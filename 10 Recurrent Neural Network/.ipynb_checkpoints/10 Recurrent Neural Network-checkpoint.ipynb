{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "# Content\n",
    "\n",
    "1. [Import Related Modules and Packages](#Sec1)\n",
    "2. [Hyperparameters](#Sec2)\n",
    "3. [Prepare for data](#Sec3)\n",
    "4. [Build RNN from scratch](#Sec4)\n",
    "5. [Build RNN from TF modules](#Sec5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Related Modules and Packages<a id='Sec1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "tfe = tf.contrib.eager\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameters<a id='Sec2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "total_series_length = 50000\n",
    "truncated_backprop_length = 15\n",
    "state_size = 4\n",
    "num_classes = 2\n",
    "echo_step = 3\n",
    "batch_size = 5\n",
    "num_batches = total_series_length//batch_size//truncated_backprop_length\n",
    "learning_rate = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare for data<a id='Sec3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(total_series_length = 50000, echo_step = 3, batch_size = 5):\n",
    "    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]), dtype=np.int32)\n",
    "    y = np.roll(x, echo_step)\n",
    "    y[0:echo_step] = 0\n",
    "\n",
    "    x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n",
    "    y = y.reshape((batch_size, -1))\n",
    "\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build RNN from scratch<a id='Sec4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRNNCell(object):\n",
    "    def __init__(self, input_size, state_size, output_size, init_state):\n",
    "        # 定义输入层与隐含层间的参数并随机初始化\n",
    "        self.W = tfe.Variable(np.random.rand(input_size+1, state_size), dtype=tf.float32)\n",
    "        self.b = tfe.Variable(np.zeros((1, state_size)), dtype=tf.float32)\n",
    "        \n",
    "        self.W2 = tfe.Variable(np.random.rand(state_size, output_size),dtype=tf.float32)\n",
    "        self.b2 = tfe.Variable(np.zeros((1, output_size)), dtype=tf.float32)\n",
    "        \n",
    "        self.STATE_SIZE = state_size\n",
    "        self.state = init_state\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "        Input:\n",
    "            x - list of tensor with shape [BATCH_SIZE, ] and length TIME_STEP\n",
    "        '''\n",
    "            \n",
    "        batch_size = tf.shape(x[0])[0]\n",
    "        \n",
    "        states_series = []\n",
    "        for current_input in x:\n",
    "            current_input = tf.reshape(current_input, [batch_size, 1])\n",
    "            input_and_state_concatenated = tf.concat([current_input, self.state], 1)  # Increasing number of columns\n",
    "\n",
    "            next_state = tf.tanh(tf.matmul(input_and_state_concatenated, self.W) + self.b)  # Broadcasted addition\n",
    "            states_series.append(next_state)\n",
    "            self.state = next_state\n",
    "        logits_series = tf.stack([tf.matmul(state, self.W2) + self.b2 for state in states_series]) #Broadcasted addition\n",
    "        predictions_series = tf.nn.softmax(logits_series)\n",
    "        return predictions_series\n",
    "\n",
    "def loss(y_pred, y_true):\n",
    "    '''\n",
    "    Input:\n",
    "        y_pred - [BATCH_SIZE * STEP_ZIE, NUM_CLASS]\n",
    "        y      - [BATCH_SIZE * STEP_ZIE, NUM_CLASS]\n",
    "    '''\n",
    "    eps = 1e-6\n",
    "    cliped_y_pref_tf = tf.clip_by_value(y_pred, eps, 1-eps)\n",
    "    losses = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(cliped_y_pref_tf), axis=1))\n",
    "    # losses = tf.losses.sparse_softmax_cross_entropy(y, y_pred)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data, epoch 0\n",
      "Step 0 Loss 0.75769806\n",
      "Step 100 Loss 0.6888367\n",
      "Step 200 Loss 0.6951802\n",
      "Step 300 Loss 0.7014157\n",
      "Step 400 Loss 0.6878339\n",
      "Step 500 Loss 0.7052706\n",
      "Step 600 Loss 0.69145566\n",
      "New data, epoch 1\n",
      "Step 0 Loss 0.67821056\n",
      "Step 100 Loss 0.69223183\n",
      "Step 200 Loss 0.67115676\n",
      "Step 300 Loss 0.6947225\n",
      "Step 400 Loss 0.7136558\n",
      "Step 500 Loss 0.69313484\n",
      "Step 600 Loss 0.7418151\n",
      "New data, epoch 2\n",
      "Step 0 Loss 0.71476513\n",
      "Step 100 Loss 0.6844871\n",
      "Step 200 Loss 0.7002073\n",
      "Step 300 Loss 0.69965464\n",
      "Step 400 Loss 0.7054781\n",
      "Step 500 Loss 0.6978116\n",
      "Step 600 Loss 0.694739\n",
      "New data, epoch 3\n",
      "Step 0 Loss 0.68291354\n",
      "Step 100 Loss 0.69485235\n",
      "Step 200 Loss 0.6941126\n",
      "Step 300 Loss 0.6984619\n",
      "Step 400 Loss 0.6907612\n",
      "Step 500 Loss 0.6807467\n",
      "Step 600 Loss 0.69389987\n",
      "New data, epoch 4\n",
      "Step 0 Loss 0.689857\n",
      "Step 100 Loss 0.6923895\n",
      "Step 200 Loss 0.6955769\n",
      "Step 300 Loss 0.70237267\n",
      "Step 400 Loss 0.6767152\n",
      "Step 500 Loss 0.7047405\n",
      "Step 600 Loss 0.70090014\n",
      "New data, epoch 5\n",
      "Step 0 Loss 0.7109435\n",
      "Step 100 Loss 0.7033527\n",
      "Step 200 Loss 0.69473016\n",
      "Step 300 Loss 0.7135542\n",
      "Step 400 Loss 0.6417124\n",
      "Step 500 Loss 0.4614068\n",
      "Step 600 Loss 0.01808035\n",
      "New data, epoch 6\n",
      "Step 0 Loss 0.2646233\n",
      "Step 100 Loss 0.009605774\n",
      "Step 200 Loss 0.0060529723\n",
      "Step 300 Loss 0.0034266582\n",
      "Step 400 Loss 0.0031646625\n",
      "Step 500 Loss 0.002440983\n",
      "Step 600 Loss 0.001997698\n",
      "New data, epoch 7\n",
      "Step 0 Loss 0.31945595\n",
      "Step 100 Loss 0.012165833\n",
      "Step 200 Loss 0.0064244717\n",
      "Step 300 Loss 0.003937736\n",
      "Step 400 Loss 0.0029148853\n",
      "Step 500 Loss 0.0025680368\n",
      "Step 600 Loss 0.001985487\n",
      "New data, epoch 8\n",
      "Step 0 Loss 0.24799103\n",
      "Step 100 Loss 0.0016407305\n",
      "Step 200 Loss 0.001471235\n",
      "Step 300 Loss 0.0013059461\n",
      "Step 400 Loss 0.0011190622\n",
      "Step 500 Loss 0.0010593456\n",
      "Step 600 Loss 0.0009766662\n",
      "New data, epoch 9\n",
      "Step 0 Loss 0.3360042\n",
      "Step 100 Loss 0.0009277292\n",
      "Step 200 Loss 0.00081482774\n",
      "Step 300 Loss 0.0007496002\n",
      "Step 400 Loss 0.00075352547\n",
      "Step 500 Loss 0.0006279333\n",
      "Step 600 Loss 0.0006704257\n",
      "New data, epoch 10\n",
      "Step 0 Loss 0.50451887\n",
      "Step 100 Loss 0.0006602735\n",
      "Step 200 Loss 0.00063079013\n",
      "Step 300 Loss 0.0005317955\n",
      "Step 400 Loss 0.00054906844\n",
      "Step 500 Loss 0.0004996266\n",
      "Step 600 Loss 0.00047155342\n",
      "New data, epoch 11\n",
      "Step 0 Loss 0.39976597\n",
      "Step 100 Loss 0.0005315337\n",
      "Step 200 Loss 0.00052222516\n",
      "Step 300 Loss 0.00048027505\n",
      "Step 400 Loss 0.00043566618\n",
      "Step 500 Loss 0.00044406028\n",
      "Step 600 Loss 0.0004885179\n",
      "New data, epoch 12\n",
      "Step 0 Loss 0.31351417\n",
      "Step 100 Loss 0.00046070814\n",
      "Step 200 Loss 0.0004064689\n",
      "Step 300 Loss 0.0004288673\n",
      "Step 400 Loss 0.0003541582\n",
      "Step 500 Loss 0.00031302925\n",
      "Step 600 Loss 0.00036143253\n",
      "New data, epoch 13\n",
      "Step 0 Loss 0.21912883\n",
      "Step 100 Loss 0.00040075116\n",
      "Step 200 Loss 0.00035082063\n",
      "Step 300 Loss 0.00034638075\n",
      "Step 400 Loss 0.0003413774\n",
      "Step 500 Loss 0.0003199554\n",
      "Step 600 Loss 0.00029290968\n",
      "New data, epoch 14\n",
      "Step 0 Loss 0.43269745\n",
      "Step 100 Loss 0.0018188562\n",
      "Step 200 Loss 0.0010950617\n",
      "Step 300 Loss 0.001260414\n",
      "Step 400 Loss 0.0008956346\n",
      "Step 500 Loss 0.0006323956\n",
      "Step 600 Loss 0.0006685885\n",
      "New data, epoch 15\n",
      "Step 0 Loss 0.23461156\n",
      "Step 100 Loss 0.00086057663\n",
      "Step 200 Loss 0.00076540804\n",
      "Step 300 Loss 0.0006410449\n",
      "Step 400 Loss 0.0006703508\n",
      "Step 500 Loss 0.00064118067\n",
      "Step 600 Loss 0.00046721494\n",
      "New data, epoch 16\n",
      "Step 0 Loss 0.26295224\n",
      "Step 100 Loss 0.0005793155\n",
      "Step 200 Loss 0.0006606187\n",
      "Step 300 Loss 0.00048992236\n",
      "Step 400 Loss 0.00041015036\n",
      "Step 500 Loss 0.00041558535\n",
      "Step 600 Loss 0.00056209054\n",
      "New data, epoch 17\n",
      "Step 0 Loss 0.3787912\n",
      "Step 100 Loss 0.00042355523\n",
      "Step 200 Loss 0.00039125263\n",
      "Step 300 Loss 0.00039905586\n",
      "Step 400 Loss 0.00035682245\n",
      "Step 500 Loss 0.00035597652\n",
      "Step 600 Loss 0.00030879976\n",
      "New data, epoch 18\n",
      "Step 0 Loss 0.33827147\n",
      "Step 100 Loss 0.0003068527\n",
      "Step 200 Loss 0.00027629753\n",
      "Step 300 Loss 0.00028852842\n",
      "Step 400 Loss 0.00030465945\n",
      "Step 500 Loss 0.0002383096\n",
      "Step 600 Loss 0.00026255258\n",
      "New data, epoch 19\n",
      "Step 0 Loss 0.23918584\n",
      "Step 100 Loss 0.0003625532\n",
      "Step 200 Loss 0.00026179632\n",
      "Step 300 Loss 0.00031365926\n",
      "Step 400 Loss 0.00027103152\n",
      "Step 500 Loss 0.00022629231\n",
      "Step 600 Loss 0.00027974386\n",
      "New data, epoch 20\n",
      "Step 0 Loss 0.23462448\n",
      "Step 100 Loss 0.00025112863\n",
      "Step 200 Loss 0.00026111366\n",
      "Step 300 Loss 0.00026356638\n",
      "Step 400 Loss 0.00024169713\n",
      "Step 500 Loss 0.00019763481\n",
      "Step 600 Loss 0.00018616492\n",
      "New data, epoch 21\n",
      "Step 0 Loss 0.12462148\n",
      "Step 100 Loss 0.00021554326\n",
      "Step 200 Loss 0.00018872476\n",
      "Step 300 Loss 0.00020834831\n",
      "Step 400 Loss 0.0002139554\n",
      "Step 500 Loss 0.00020841748\n",
      "Step 600 Loss 0.00020343687\n",
      "New data, epoch 22\n",
      "Step 0 Loss 0.31929958\n",
      "Step 100 Loss 0.000287254\n",
      "Step 200 Loss 0.0002586106\n",
      "Step 300 Loss 0.00029965703\n",
      "Step 400 Loss 0.00026380568\n",
      "Step 500 Loss 0.00024083945\n",
      "Step 600 Loss 0.00023920047\n",
      "New data, epoch 23\n",
      "Step 0 Loss 0.31118333\n",
      "Step 100 Loss 0.00022519773\n",
      "Step 200 Loss 0.00020637903\n",
      "Step 300 Loss 0.00023345381\n",
      "Step 400 Loss 0.00022767116\n",
      "Step 500 Loss 0.00022550672\n",
      "Step 600 Loss 0.00021013973\n",
      "New data, epoch 24\n",
      "Step 0 Loss 0.33136052\n",
      "Step 100 Loss 0.000188829\n",
      "Step 200 Loss 0.00017486524\n",
      "Step 300 Loss 0.0002076925\n",
      "Step 400 Loss 0.00016217503\n",
      "Step 500 Loss 0.00020019354\n",
      "Step 600 Loss 0.00018286968\n",
      "New data, epoch 25\n",
      "Step 0 Loss 0.17882833\n",
      "Step 100 Loss 0.00021386462\n",
      "Step 200 Loss 0.00017115778\n",
      "Step 300 Loss 0.00015980999\n",
      "Step 400 Loss 0.00020849066\n",
      "Step 500 Loss 0.00018100794\n",
      "Step 600 Loss 0.00017992684\n",
      "New data, epoch 26\n",
      "Step 0 Loss 0.38386837\n",
      "Step 100 Loss 0.0002291379\n",
      "Step 200 Loss 0.00020381516\n",
      "Step 300 Loss 0.00018570336\n",
      "Step 400 Loss 0.00018236575\n",
      "Step 500 Loss 0.00015240478\n",
      "Step 600 Loss 0.00017205253\n",
      "New data, epoch 27\n",
      "Step 0 Loss 0.32598147\n",
      "Step 100 Loss 0.00017866988\n",
      "Step 200 Loss 0.000139303\n",
      "Step 300 Loss 0.00017972931\n",
      "Step 400 Loss 0.00015429758\n",
      "Step 500 Loss 0.00012504499\n",
      "Step 600 Loss 0.00014295752\n",
      "New data, epoch 28\n",
      "Step 0 Loss 0.26101285\n",
      "Step 100 Loss 0.00056614424\n",
      "Step 200 Loss 0.00051324087\n",
      "Step 300 Loss 0.0005268917\n",
      "Step 400 Loss 0.000504075\n",
      "Step 500 Loss 0.00039508523\n",
      "Step 600 Loss 0.00050134823\n",
      "New data, epoch 29\n",
      "Step 0 Loss 0.5636104\n",
      "Step 100 Loss 0.0005002598\n",
      "Step 200 Loss 0.0006412451\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-e840203d76ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mstack_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_series\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_y_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m    765\u001b[0m     flat_grad = imperative_grad.imperative_grad(\n\u001b[1;32m    766\u001b[0m         \u001b[0m_default_vspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m         output_gradients=output_gradients)\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(vspace, tape, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m     61\u001b[0m   \"\"\"\n\u001b[1;32m     62\u001b[0m   return pywrap_tensorflow.TFE_Py_TapeGradient(\n\u001b[0;32m---> 63\u001b[0;31m       tape._tape, vspace, target, sources, output_gradients)  # pylint: disable=protected-access\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgrad_fn\u001b[0;34m(*orig_outputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0morig_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     result = _magic_gradient_function(op_name, attrs, num_inputs,\n\u001b[0;32m--> 147\u001b[0;31m                                       op_inputs, op_outputs, orig_outputs)\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_tracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m       print(\"Gradient for\", op_name, \"inputs\", op_inputs, \"output_grads\",\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_magic_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_AddGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m   \u001b[0msx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m   \u001b[0msy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m   \u001b[0mrx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_gradient_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m   return (array_ops.reshape(math_ops.reduce_sum(grad, rx), sx),\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m   \"\"\"\n\u001b[0;32m--> 285\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m   \"\"\"\n\u001b[0;32m--> 303\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m     if isinstance(input, (sparse_tensor.SparseTensor,\n\u001b[1;32m    305\u001b[0m                           sparse_tensor.SparseTensorValue)):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init_state = np.zeros((batch_size, state_size), dtype=np.float32)\n",
    "model = BasicRNNCell(state_size, state_size, num_classes, init_state)\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    x,y = generateData()\n",
    "    model.state = np.zeros((batch_size, state_size))\n",
    "\n",
    "    print(\"New data, epoch\", epoch_idx)\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        with tf.GradientTape() as t:\n",
    "            start_idx = batch_idx * truncated_backprop_length\n",
    "            end_idx = start_idx + truncated_backprop_length\n",
    "\n",
    "            batchX = x[:,start_idx:end_idx]\n",
    "            batchY = y[:,start_idx:end_idx]\n",
    "\n",
    "            # Unpack columns\n",
    "            inputs_series = tf.unstack(batchX, axis=1)\n",
    "            labels_series = tf.unstack(batchY, axis=1)\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(inputs_series)\n",
    "            stack_y_pred = tf.reshape(y_pred, (-1,2))\n",
    "            stack_y = tf.reshape(tf.one_hot(tf.stack(labels_series), num_classes), (-1,2))\n",
    "            losses = loss(stack_y_pred, stack_y)\n",
    "        dW, db, dW2, db2 = t.gradient(losses, [model.W, model.b, model.W2, model.b2])\n",
    "        model.W.assign_sub(learning_rate * dW)\n",
    "        model.b.assign_sub(learning_rate * db)\n",
    "        model.W2.assign_sub(learning_rate * dW2)\n",
    "        model.b2.assign_sub(learning_rate * db2)        \n",
    "\n",
    "\n",
    "        #loss_list.append(_total_loss)\n",
    "\n",
    "        if batch_idx%100 == 0:\n",
    "            print(\"Step\",batch_idx, \"Loss\", losses.numpy())\n",
    "            #plot(loss_list, _predictions_series, batchX, batchY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(loss_list, predictions_series, batchX, batchY):\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.cla()\n",
    "    plt.plot(loss_list)\n",
    "\n",
    "    for batch_series_idx in range(5):\n",
    "        one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n",
    "        single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n",
    "\n",
    "        plt.subplot(2, 3, batch_series_idx + 2)\n",
    "        plt.cla()\n",
    "        plt.axis([0, truncated_backprop_length, 0, 2])\n",
    "        left_offset = range(truncated_backprop_length)\n",
    "        plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\n",
    "        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n",
    "        plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n",
    "\n",
    "    plt.draw()\n",
    "    plt.pause(0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build RNN from TF modules<a id='Sec5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Iter 0, Testing Accuracy= 0.7221\n",
      "Iter 1, Testing Accuracy= 0.8016\n",
      "Iter 2, Testing Accuracy= 0.8763\n",
      "Iter 3, Testing Accuracy= 0.9103\n",
      "Iter 4, Testing Accuracy= 0.9223\n",
      "Iter 5, Testing Accuracy= 0.9311\n"
     ]
    }
   ],
   "source": [
    "#载入数据集\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "\n",
    "# 输入图片是28*28\n",
    "n_inputs = 28 #输入一行，一行有28个数据\n",
    "max_time = 28 #一共28行\n",
    "lstm_size = 100 #隐层单元\n",
    "n_classes = 10 # 10个分类\n",
    "batch_size = 50 #每批次50个样本\n",
    "n_batch = mnist.train.num_examples // batch_size #计算一共有多少个批次\n",
    "\n",
    "#这里的none表示第一个维度可以是任意的长度\n",
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "#正确的标签\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "#初始化权值\n",
    "weights = tf.Variable(tf.truncated_normal([lstm_size, n_classes], stddev=0.1))\n",
    "#初始化偏置值\n",
    "biases = tf.Variable(tf.constant(0.1, shape=[n_classes]))\n",
    "\n",
    "\n",
    "#定义RNN网络\n",
    "def RNN(X,weights,biases):\n",
    "    # inputs=[batch_size, max_time, n_inputs]\n",
    "    inputs = tf.reshape(X,[-1,max_time,n_inputs])\n",
    "    #定义LSTM基本CELL\n",
    "    lstm_cell = tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(lstm_size)\n",
    "    # final_state[0]是cell state\n",
    "    # final_state[1]是hidden_state\n",
    "    outputs,final_state = tf.nn.dynamic_rnn(lstm_cell,inputs,dtype=tf.float32)\n",
    "    results = tf.nn.softmax(tf.matmul(final_state[1],weights) + biases)\n",
    "    return results\n",
    "    \n",
    "    \n",
    "#计算RNN的返回结果\n",
    "prediction= RNN(x, weights, biases)  \n",
    "#损失函数\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "#使用AdamOptimizer进行优化\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "#结果存放在一个布尔型列表中\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax返回一维张量中最大的值所在的位置\n",
    "#求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))#把correct_prediction变为float32类型\n",
    "#初始化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(6):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys})\n",
    "        \n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "        print (\"Iter \" + str(epoch) + \", Testing Accuracy= \" + str(acc))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
