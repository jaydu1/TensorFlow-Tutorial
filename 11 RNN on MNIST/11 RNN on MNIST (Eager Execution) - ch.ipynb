{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tfe = tf.contrib.eager\n",
    "# 启动eager execution模式\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A time step inputs one row of a image\n",
    "# Number of time steps = number of rows\n",
    "TIME_STEPS = 28\n",
    "# Input  size = number of columns\n",
    "INPUT_SIZE = 28\n",
    "# Size of hidden states / RNN units\n",
    "HIDDEN_SIZE = 50\n",
    "# Output size = number of classes\n",
    "OUTPUT_SIZE = 10\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 50\n",
    "# Number of epoches\n",
    "NUM_EPOCH = 1\n",
    "\n",
    "# Learning rate\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "       Data :\t shape: (60000, 28, 28) \t type: uint8\n",
      "       Label:\t shape: (60000,) \t\t type: uint8\n",
      "Testing set :\n",
      "       Data :\t shape: (10000, 28, 28) \t type: uint8\n",
      "       Label:\t shape: (10000,) \t\t type: uint8\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
    "\n",
    "print('Training set:')\n",
    "print('       Data :\\t shape:', np.shape(x_train), '\\t type:', x_train.dtype)\n",
    "print('       Label:\\t shape:', np.shape(y_train), '\\t\\t type:', y_train.dtype)\n",
    "print('Testing set :')\n",
    "print('       Data :\\t shape:', np.shape(x_test), '\\t type:', x_test.dtype)\n",
    "print('       Label:\\t shape:', np.shape(y_test), '\\t\\t type:', y_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]], shape=(60000, 10), dtype=float32) tf.Tensor(\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(10000, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 转化为 float 型并归一化\n",
    "x_train = x_train.astype(np.float32)/255\n",
    "x_test = x_test.astype(np.float32)/255\n",
    "print(np.shape(x_train), np.shape(x_test))\n",
    "\n",
    "# 标签转化为 ont hot 向量\n",
    "y_train = tf.one_hot(y_train, 10, dtype=tf.float32)\n",
    "y_test = tf.one_hot(y_test, 10, dtype=tf.float32)\n",
    "print(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. 生成Dataset对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training Dataset\n",
    "TrainDataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# Shuffle per buffer_size\n",
    "TrainDataset = TrainDataset.shuffle(buffer_size=5000)\n",
    "# Batch size\n",
    "TrainDataset = TrainDataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# Generate testing Dataset\n",
    "TestDataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Build RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.units = hidden_size\n",
    "        \n",
    "        self.rnn = tf.keras.layers.SimpleRNN(units = hidden_size,   # units = HIDDEN_SIZE\n",
    "                                             unroll = True,         # Allocate and fix the shape \n",
    "                                             # of inputs, eg [NUM_BATCH, TIME_STEPS, INPUT_SIZE]\n",
    "                                             # in our case. Unrolling can speed-up a RNN, although \n",
    "                                             # it tends to be more memory-intensive. Unrolling is \n",
    "                                             # only suitable for short sequences.\n",
    "                                             stateful = False,      # The last state of current \n",
    "                                             # batched samples won't be used as the initial state \n",
    "                                             # in the next batch. Only use when samples are \n",
    "                                             # truncated successively from a long sequences.\n",
    "                                             return_sequences = False# Only return the last state.\n",
    "                                            )\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(output_size)\n",
    "        #self.softmax = tf.keras.layers.Softmax()\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # output at every time step\n",
    "        # output shape == (batch_size, seq_length, hidden_size) \n",
    "        output = self.rnn(inputs)\n",
    "\n",
    "        # The dense layer will output predictions for every time_steps(seq_length)\n",
    "        # output shape after the dense layer == (seq_length * batch_size, vocab_size)\n",
    "        prediction = self.fc(output)\n",
    "        \n",
    "#         if is_prob:\n",
    "#             prediction = self.softmax(fc)\n",
    "        # states will be used to pass at every step to the model while training\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\n",
    "def Loss(y_pred, y_true):\n",
    "    '''\n",
    "    Input:\n",
    "        y_pred - [BATCH_SIZE, NUM_CLASS]\n",
    "        y      - [BATCH_SIZE, NUM_CLASS]\n",
    "    '''    \n",
    "    return tf.losses.softmax_cross_entropy(onehot_labels=y_true, logits=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Accuracy Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(y_pred, y_true):\n",
    "    '''\n",
    "    Input:\n",
    "        y_pred - [BATCH_SIZE, NUM_CLASS]\n",
    "        y      - [BATCH_SIZE, NUM_CLASS]\n",
    "    '''\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_true,1), tf.argmax(y_pred,1)),tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using adam optimizer with default arguments\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_3 (SimpleRNN)     multiple                  3950      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  510       \n",
      "=================================================================\n",
      "Total params: 4,460\n",
      "Trainable params: 4,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "# Build the model so that \n",
    "#     1. it can be printed by model.summary() \n",
    "#     2. reset_states() can be called\n",
    "# In tf 1.11, we can directly call model.build(INPUT_SHAPE) to \n",
    "# build the model, but in early version, we have to feed a \n",
    "# dummy input to build it:\n",
    "#    dummy_x = tf.zeros((BATCH_SIZE, TIME_STEPS, INPUT_SIZE))\n",
    "#    model._set_inputs(dummy_x)\n",
    "\n",
    "# For unroll=True:\n",
    "model.build((NUM_BATCH, TIME_STEPS, INPUT_SIZE))\n",
    "# For unroll=False:\n",
    "#    model.build((None, TIME_STEPS, INPUT_SIZE))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.4750\n",
      "Epoch 1 Batch 500 Loss 0.5501\n",
      "Epoch 1 Batch 1000 Loss 0.4739\n",
      "Epoch 1 Loss 0.1727\n",
      "Time taken for 1 epoch 42.26512694358826 sec\n",
      "\n",
      "test cost:  0.30884752 test accuracy:  0.90799993\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    start = time.time()\n",
    "    \n",
    "    # initializing the hidden state at the start of every epoch\n",
    "    # initally hidden is None\n",
    "    hidden = model.reset_states()\n",
    "    \n",
    "    for (batch, (x, y)) in enumerate(TrainDataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # feeding the hidden state back into the model\n",
    "            # This is the interesting step\n",
    "            y_pred = model(x)\n",
    "            loss = Loss(y_pred, y)\n",
    "\n",
    "        grads = tape.gradient(loss, model.variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables))\n",
    "\n",
    "        if batch % 500 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
    "                                                          batch,\n",
    "                                                          loss))\n",
    "           \n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    \n",
    "    # For unroll=True:\n",
    "    cost = []\n",
    "    accuracy = []    \n",
    "    for x, y in TestDataset:\n",
    "        y_pred = model(x)\n",
    "        cost.append(Loss(y_pred, y))\n",
    "        accuracy.append(Accuracy(y_pred, y))\n",
    "    # For unroll=False:\n",
    "    #    y_pred = model(x_test)\n",
    "    #    cost = Loss(y_pred, y_test)\n",
    "    #    accuracy = Accuracy(y_pred, y_test)\n",
    "    print('test cost: ', np.mean(cost), 'test accuracy: ', np.mean(accuracy))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
