{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tfe = tf.contrib.eager\n",
    "# 启动eager execution模式\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总时间步长 = 图像行数\n",
    "TIME_STEPS = 28\n",
    "# 输入维度  = 图像列数\n",
    "INPUT_SIZE = 28\n",
    "# 隐含状态单元数\n",
    "HIDDEN_SIZE = 50\n",
    "# 输出维度  = 图像类别数\n",
    "OUTPUT_SIZE = 10\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 50\n",
    "# epoches数\n",
    "NUM_EPOCH = 1\n",
    "\n",
    "# 学习率\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "       Data :\t shape: (60000, 28, 28) \t type: uint8\n",
      "       Label:\t shape: (60000,) \t\t type: uint8\n",
      "Testing set :\n",
      "       Data :\t shape: (10000, 28, 28) \t type: uint8\n",
      "       Label:\t shape: (10000,) \t\t type: uint8\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
    "\n",
    "print('Training set:')\n",
    "print('       Data :\\t shape:', np.shape(x_train), '\\t type:', x_train.dtype)\n",
    "print('       Label:\\t shape:', np.shape(y_train), '\\t\\t type:', y_train.dtype)\n",
    "print('Testing set :')\n",
    "print('       Data :\\t shape:', np.shape(x_test), '\\t type:', x_test.dtype)\n",
    "print('       Label:\\t shape:', np.shape(y_test), '\\t\\t type:', y_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]], shape=(60000, 10), dtype=float32) tf.Tensor(\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(10000, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 转化为 float 型并归一化\n",
    "x_train = x_train.astype(np.float32)/255\n",
    "x_test = x_test.astype(np.float32)/255\n",
    "print(np.shape(x_train), np.shape(x_test))\n",
    "\n",
    "# 标签转化为 ont hot 向量\n",
    "y_train = tf.one_hot(y_train, 10, dtype=tf.float32)\n",
    "y_test = tf.one_hot(y_test, 10, dtype=tf.float32)\n",
    "print(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. 生成Dataset对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成用于训练的Dataset对象\n",
    "TrainDataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# 按buffer_size打乱\n",
    "TrainDataset = TrainDataset.shuffle(buffer_size=5000)\n",
    "# Batch size\n",
    "TrainDataset = TrainDataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# 生成用于测试的Dataset对象\n",
    "TestDataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 创建网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. 创建RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.units = hidden_size\n",
    "        \n",
    "        self.rnn = tf.keras.layers.SimpleRNN(units = hidden_size,   # units = HIDDEN_SIZE\n",
    "                                             unroll = True,         # 预分配输入的形状，这里我们设置为\n",
    "                                             # [NUM_BATCH, TIME_STEPS, INPUT_SIZE]。可以加快训练，但\n",
    "                                             # 会消耗大量内存，只有当序列较短时适用。\n",
    "                                             stateful = False,      # 当前batch的最后一个状态不会用于\n",
    "                                             # 下一个batch的初始状态，只有当数据是依次截断的时候适用。\n",
    "                                             return_sequences = False# 只返回最后一个时间的状态\n",
    "                                            )\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(output_size)\n",
    "        # 如果需要输出概率，添加：\n",
    "        # self.softmax = tf.keras.layers.Softmax()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # output shape == (BATCH_SIZE, HIDDEN_SIZE) \n",
    "        output = self.rnn(inputs)\n",
    "\n",
    "        # output shape == (BATCH_SIZE, OUTPUT_SIZE)\n",
    "        prediction = self.fc(output)\n",
    "        \n",
    "        # 如果需要输出概率，添加：\n",
    "        #     prediction = self.softmax(fc)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(y_pred, y_true):\n",
    "    '''\n",
    "    Input:\n",
    "        y_pred - [BATCH_SIZE, NUM_CLASS]\n",
    "        y      - [BATCH_SIZE, NUM_CLASS]\n",
    "    '''    \n",
    "    # 如果标签不是one-hot向量，使用sparse_softmax_cross_entropy\n",
    "    return tf.losses.softmax_cross_entropy(onehot_labels=y_true, logits=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(y_pred, y_true):\n",
    "    '''\n",
    "    Input:\n",
    "        y_pred - [BATCH_SIZE, NUM_CLASS]\n",
    "        y      - [BATCH_SIZE, NUM_CLASS]\n",
    "    '''\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_true,1), tf.argmax(y_pred,1)),tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用adam优化器，默认参数\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_3 (SimpleRNN)     multiple                  3950      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  510       \n",
      "=================================================================\n",
      "Total params: 4,460\n",
      "Trainable params: 4,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "# 初始化模型才可以：\n",
    "#     1. 调用model.summary() \n",
    "#     2. 调用reset_states()\n",
    "# 在tf 1.11可用直接调用model.build(INPUT_SHAPE) 来初始化模型，但\n",
    "# 在早期版本，可能需要输入一个哑变量：\n",
    "#    dummy_x = tf.zeros((BATCH_SIZE, TIME_STEPS, INPUT_SIZE))\n",
    "#    model._set_inputs(dummy_x)\n",
    "\n",
    "# 若 unroll=True:\n",
    "model.build((NUM_BATCH, TIME_STEPS, INPUT_SIZE))\n",
    "# 若 unroll=False:\n",
    "#    model.build((None, TIME_STEPS, INPUT_SIZE))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.4750\n",
      "Epoch 1 Batch 500 Loss 0.5501\n",
      "Epoch 1 Batch 1000 Loss 0.4739\n",
      "Epoch 1 Loss 0.1727\n",
      "Time taken for 1 epoch 42.26512694358826 sec\n",
      "\n",
      "test cost:  0.30884752 test accuracy:  0.90799993\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    start = time.time()\n",
    "    \n",
    "    # 在每个epoch开始时初始化隐藏状态，一开始为None\n",
    "    hidden = model.reset_states()\n",
    "    \n",
    "    for (batch, (x, y)) in enumerate(TrainDataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 前向传播\n",
    "            y_pred = model(x)\n",
    "            loss = Loss(y_pred, y)\n",
    "        # 反向传播\n",
    "        grads = tape.gradient(loss, model.variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables))\n",
    "\n",
    "        if batch % 500 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
    "                                                          batch,\n",
    "                                                          loss))\n",
    "           \n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    \n",
    "    # 若 unroll=True:\n",
    "    cost = []\n",
    "    accuracy = []    \n",
    "    for x, y in TestDataset:\n",
    "        y_pred = model(x)\n",
    "        cost.append(Loss(y_pred, y))\n",
    "        accuracy.append(Accuracy(y_pred, y))\n",
    "    # For若 unroll=False:\n",
    "    #    y_pred = model(x_test)\n",
    "    #    cost = Loss(y_pred, y_test)\n",
    "    #    accuracy = Accuracy(y_pred, y_test)\n",
    "    print('test cost: ', np.mean(cost), 'test accuracy: ', np.mean(accuracy))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
