{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用胶囊网络解决MNIST分类问题\n",
    "- 作者 : *Jinhong Du*\n",
    "\n",
    "- 参考资料: \n",
    "    \n",
    "    1. https://arxiv.org/pdf/1710.09829.pdf\n",
    "    2. https://github.com/naturomics/CapsNet-Tensorflow/\n",
    "    3. https://www.jiqizhixin.com/articles/2017-11-05\n",
    "\n",
    "# 目录\n",
    "\n",
    "1. [导入相关包](#Sec1)\n",
    "2. [设置超参数](#Sec2)\n",
    "3. [准备数据](#Sec3)\n",
    "    - [导入数据](#Sec3.1)\n",
    "    - [数据预处理](#Sec3.1)\n",
    "    - [生成Dataset](#Sec3.1) \n",
    "4. [建立模型](#Sec4)\n",
    "    - [Squash函数](#Sec4.1)\n",
    "    - [Routing函数](#Sec4.2)\n",
    "    - [Primary Capsules](#Sec4.3)\n",
    "    - [Digit Capsules](#Sec4.4)\n",
    "    - [建立CapsNet模型](#Sec4.5)\n",
    "    - [损失函数](#Sec4.6) \n",
    "    - [优化器](#Sec4.7)\n",
    "    - [训练与评估](#Sec4.8)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入相关包<a id='Sec1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 启动eager execution模式，必须在项目开始时运行\n",
    "import tensorflow as tf\n",
    "tfe = tf.contrib.eager\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 设置超参数<a id='Sec2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASS = 10\n",
    "NUM_EPOCH = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 准备数据<a id='Sec3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. 导入数据<a id='Sec3.1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "       Data :\t shape: (60000, 28, 28) \t type: uint8\n",
      "       Label:\t shape: (60000,) \t\t type: uint8\n",
      "Testing set :\n",
      "       Data :\t shape: (10000, 28, 28) \t type: uint8\n",
      "       Label:\t shape: (10000,) \t\t type: uint8\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path=r'C:\\Users\\Administrator\\Downloads\\mnist.npz')\n",
    "\n",
    "print('Training set:')\n",
    "print('       Data :\\t shape:', np.shape(x_train), '\\t type:', x_train.dtype)\n",
    "print('       Label:\\t shape:', np.shape(y_train), '\\t\\t type:', y_train.dtype)\n",
    "print('Testing set :')\n",
    "print('       Data :\\t shape:', np.shape(x_test), '\\t type:', x_test.dtype)\n",
    "print('       Label:\\t shape:', np.shape(y_test), '\\t\\t type:', y_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. 数据预处理<a id='Sec3.2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (10000, 28, 28, 1)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]], shape=(60000, 10), dtype=float32) tf.Tensor(\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(10000, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 转化为浮点型并归一化\n",
    "x_train = np.reshape(x_train.astype(np.float32)/255,(-1,28,28,1))\n",
    "x_test = np.reshape(x_test.astype(np.float32)/255,(-1,28,28,1))\n",
    "print(np.shape(x_train), np.shape(x_test))\n",
    "\n",
    "# 将标签转化为ont-hot向量\n",
    "y_train = tf.one_hot(y_train, NUM_CLASS, dtype=tf.float32)\n",
    "y_test = tf.one_hot(y_test, NUM_CLASS, dtype=tf.float32)\n",
    "print(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. 生成Dataset<a id='Sec3.3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成用于训练的Dataset\n",
    "TrainDataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# 按buffer_size打乱\n",
    "TrainDataset = TrainDataset.shuffle(buffer_size=5000)\n",
    "# 设置Batch size，若最后一个batch不足BATCH_SIZE个则丢弃\n",
    "TrainDataset = TrainDataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# 生成用于测试的Dataset\n",
    "TestDataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 建立模型<a id='Sec4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Squash函数<a id='Sec4.1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(vector):\n",
    "    '''Squashing函数\n",
    "    Args:\n",
    "        vector: A tensor with shape [batch_size, 1, num_caps, vec_len, 1] or [batch_size, num_caps, vec_len, 1].\n",
    "    Returns:\n",
    "        A tensor with the same shape as vector but squashed in 'vec_len' dimension.\n",
    "    '''\n",
    "    epsilon = 1e-9\n",
    "    # 计算向量的模\n",
    "    vec_squared_norm = tf.reduce_sum(tf.square(vector), -2, keepdims=True)\n",
    "    # 计算标准化的系数\n",
    "    scalar_factor = vec_squared_norm / (1 + vec_squared_norm) / tf.sqrt(vec_squared_norm + epsilon)\n",
    "    # 逐元素乘积\n",
    "    vec_squashed = scalar_factor * vector  \n",
    "    return(vec_squashed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Routing函数<a id='Sec4.2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def routing(input_tensor, b, W, biases, num_caps_out=10, num_dims=16, iteration=3):\n",
    "    ''' The routing algorithm.\n",
    "    Args:\n",
    "        input_tensor - A tensor with [batch_size, num_caps_in=1152, 1, length(u_i)=8, 1]\n",
    "                        shape, num_caps_in meaning the number of capsule in layer l, i.e. \n",
    "                        the preceding layer.\n",
    "        b            - A tensor with all values being zeros and [1, num_caps_in, \n",
    "                        num_caps_out] shape. \n",
    "        W            - A tensor with [1, num_caps_in, num_caps_out * len_v_j, len_u_j, 1]\n",
    "                        shape. \n",
    "        num_caps_out - The number of output capsules.\n",
    "        num_dims     - The number of dimensions for output capsule.\n",
    "    Returns:\n",
    "        A Tensor of shape [batch_size, num_caps_l_plus_1, length(v_j)=16, 1]\n",
    "        representing the vector output `v_j` in the layer l+1\n",
    "    Notes:\n",
    "        u_i represents the vector output of capsule i in the layer l, and\n",
    "        v_j the vector output of capsule j in the layer l+1.\n",
    "     '''\n",
    "\n",
    "    input_shape = tf.shape(input_tensor)\n",
    "    # [batch_size, 1152, 160, 8, 1]\n",
    "    \n",
    "    # Eq.2, 计算u_hat\n",
    "    # 因为tf.matmul操作比较耗时，我们可以尝试用逐元素相乘来计算：\n",
    "    # 形状为[a, b]、[b, c]的两矩阵相乘 <=> 先将形状复制为[a*c, b]、[b, a*c]，再将第二个矩阵转置，\n",
    "    # 两者逐元素相乘后再对axis=1求和，并reshape为[a, c]\n",
    "    input_tensor = tf.tile(input_tensor, [1, 1, num_dims * num_caps_out, 1, 1])\n",
    "    u_hat = tf.reduce_sum(W * input_tensor, axis=3, keepdims=True)\n",
    "    u_hat = tf.reshape(u_hat, shape=[-1, input_shape[1], num_caps_out, num_dims, 1])\n",
    "    # [batch_size, 1152, 10, 16, 1]\n",
    "\n",
    "    # u_hat_stopped = u_hat用于routing算法，不计算其梯度\n",
    "    u_hat_stopped = tf.stop_gradient(u_hat, name='stop_gradient')\n",
    "\n",
    "    # line 3, 开始迭代\n",
    "    for r_iter in range(iteration):\n",
    "        # line 4:\n",
    "        c_IJ = tf.nn.softmax(b, axis=2)\n",
    "        # [batch_size, 1152, 10, 1, 1]\n",
    "        \n",
    "        # 在最后一次迭代，用`u_hat`来连接后续网络\n",
    "        if r_iter == iteration - 1:\n",
    "            # line 5:\n",
    "            # 用c_IJ作为权重计算u_hat的加权和s_J, c_IJ*u_hat最后两个维度逐元素相乘 \n",
    "            s_J = tf.multiply(c_IJ, u_hat)\n",
    "            # [batch_size, 1152, 10, 16, 1]\n",
    "            \n",
    "            s_J = tf.reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
    "            # [batch_size, 1, 10, 16, 1]\n",
    "\n",
    "            # line 6:\n",
    "            # squash\n",
    "            v_J = squash(s_J)\n",
    "            # [batch_size, 1, 10, 16, 1]\n",
    "            \n",
    "        # 最后一次迭代之前不需要反向传播误差，用u_hat_stopped\n",
    "        elif r_iter < iteration - 1:  \n",
    "            s_J = tf.multiply(c_IJ, u_hat_stopped)\n",
    "            s_J = tf.reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
    "            v_J = squash(s_J)\n",
    "\n",
    "            # line 7:\n",
    "            # reshape & tile v_j from [batch_size ,1, 10, 16, 1] to [batch_size, 1152, 10, 16, 1]\n",
    "            v_J_tiled = tf.tile(v_J, [1, input_shape[1], 1, 1, 1])\n",
    "            u_produce_v = tf.reduce_sum(u_hat_stopped * v_J_tiled, axis=3, keepdims=True)\n",
    "            # [batch_size, 1152, 10, 1, 1]\n",
    "\n",
    "            b += u_produce_v\n",
    "\n",
    "    return(v_J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Primary Capsules<a id='Sec4.3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps(object):\n",
    "    def __init__(self, num_caps_out=32, vec_len=8):\n",
    "        self.num_caps_out = num_caps_out\n",
    "        self.vec_len = vec_len\n",
    "        self.caps = [\n",
    "            tf.layers.Conv2D(filters = self.num_caps_out, kernel_size = 9, strides = 2,\n",
    "                             padding = 'valid', activation = None, name = 'PrimaryCaps_%d'%i) for i in range(self.vec_len)]\n",
    "    \n",
    "    def __call__(self, x):  \n",
    "        batch_size = tf.shape(x)[0]\n",
    "        capsules = []\n",
    "        for i in range(self.vec_len):\n",
    "            # 将一般卷积的结果张量拉平，并为添加到列表中\n",
    "            caps_i = tf.reshape(self.caps[i](x), shape=(batch_size, -1, 1, 1))\n",
    "            capsules.append(caps_i)\n",
    "\n",
    "        # 合并为PrimaryCaps的输出张量，即6×6×32个长度为8的向量，合并后的维度为 [batch_size, 1152, 8, 1]\n",
    "        capsules = tf.concat(capsules, axis=2)\n",
    "        # 将每个Capsule 向量投入非线性函数squash进行缩放与激活\n",
    "        capsules = squash(capsules)\n",
    "        return capsules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Digit Capsules<a id='Sec4.4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitCaps(object):\n",
    "    def __init__(self, num_caps_out=10, vec_len=16):\n",
    "        self.num_caps_out = num_caps_out\n",
    "        self.vec_len = vec_len\n",
    "        self.input_shape = [-1, 1152, 8, 1]\n",
    "        \n",
    "        self.W = tfe.Variable(\n",
    "            tf.random_normal(shape=[1, self.input_shape[1], self.vec_len * self.num_caps_out] + self.input_shape[-2:],\n",
    "            dtype=tf.float32, stddev=0.01), name = 'DigitCaps_W')\n",
    "        self.biases = tfe.Variable(\n",
    "            tf.zeros(shape=(1, 1, self.num_caps_out, self.vec_len, 1),\n",
    "            dtype=tf.float32), name = 'DigitCaps_b')\n",
    "    \n",
    "    def __call__(self, input_tensor): \n",
    "        batch_size = tf.shape(input_tensor)[0]\n",
    "        input_tensor = tf.reshape(input_tensor, shape=(batch_size, -1, 1, self.input_shape[-2], 1))\n",
    "        b = tf.constant(tf.zeros([1, self.input_shape[1], self.num_caps_out, 1, 1], dtype=np.float32))\n",
    "        capsules = routing(input_tensor, b, self.W, self.biases)\n",
    "        # tf.squeeze去除为1的维度得到 DigitCaps 层的输出向量\n",
    "        capsules = tf.squeeze(capsules, axis=1)\n",
    "        return capsules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. 建立CapsNet模型<a id='Sec4.5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet(tf.keras.Model):\n",
    "    def __init__(self, height=28, width=28, channels=1, num_label=NUM_CLASS):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channels = channels\n",
    "        self.num_label = num_label\n",
    "        \n",
    "        # 1st layer\n",
    "        self.Conv1 = tf.layers.Conv2D(filters = 256, kernel_size = 9, strides = 1,\n",
    "                                      padding = 'valid', activation = tf.nn.relu, name = 'Conv1')\n",
    "        \n",
    "        # 2nd layer - PrimaryCaps\n",
    "        self.PrimaryCaps = PrimaryCaps(num_caps_out=32, vec_len=8)\n",
    "  \n",
    "        # 3rd layer - DigitCaps\n",
    "        self.DigitCaps = DigitCaps(num_caps_out=10, vec_len=16)\n",
    "        \n",
    "        # 重构部分\n",
    "        # 3个全连接层\n",
    "        self.fc1 = tf.layers.Dense(\n",
    "                            units=512,\n",
    "                            activation=tf.nn.relu,\n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                            bias_initializer=tf.zeros_initializer(),\n",
    "                            name='fc1')\n",
    "        self.fc2 = tf.layers.Dense(\n",
    "                            units=1024,\n",
    "                            activation=tf.nn.relu,\n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                            bias_initializer=tf.zeros_initializer(),\n",
    "                            name='fc2')    \n",
    "        self.fc3 = tf.layers.Dense(\n",
    "                            units=self.height * self.width * self.channels,\n",
    "                            activation=tf.sigmoid,\n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                            bias_initializer=tf.zeros_initializer(),\n",
    "                            name='fc3')\n",
    "        \n",
    "    def call(self, X, Y, reconstruct=False):\n",
    "        epsilon = 1e-9\n",
    "        batch_size = tf.shape(X)[0]\n",
    "        \n",
    "        conv1 = self.Conv1(X)\n",
    "        # Output [batch_size, 1152, 8, 1]\n",
    "        caps1 = self.PrimaryCaps(conv1)\n",
    "        v = self.DigitCaps(caps1)\n",
    "        \n",
    "        # a). 计算v的模||v_c||，及softmax(||v_c||)\n",
    "        # [batch_size, 10, 16, 1] => [batch_size, 10, 1, 1]\n",
    "        v_length = tf.sqrt(tf.reduce_sum(tf.square(v),\n",
    "                                               axis=2, keepdims=True) + epsilon)\n",
    "        softmax_v = tf.nn.softmax(v_length, axis=1)\n",
    "        # [batch_size, self.num_label, 1, 1]\n",
    "\n",
    "        # b). 取出模最大对应的索引\n",
    "        # [batch_size, 10, 1, 1] => [batch_size] (index)\n",
    "        argmax_idx = tf.to_int32(tf.argmax(softmax_v, axis=1))\n",
    "        # [batch_size, 1, 1]\n",
    "        argmax_idx = tf.reshape(argmax_idx, shape=(batch_size, ))\n",
    "            \n",
    "        masked_v = tf.multiply(tf.squeeze(v), tf.reshape(Y, (-1, self.num_label, 1)))\n",
    "        \n",
    "        # 2. 重构\n",
    "        # [batch_size, 1, 16, 1] => [batch_size, 16] => [batch_size, 512]\n",
    "        vector_j = tf.reshape(masked_v, shape=(batch_size, -1))\n",
    "        fc1_output = self.fc1(vector_j)\n",
    "        fc2_output = self.fc2(fc1_output)\n",
    "        X_decoded = self.fc3(fc2_output)\n",
    "        margin_loss, self.reconstruction_loss, self.total_loss = Loss(X, Y, v, X_decoded)\n",
    "        \n",
    "        if reconstruct:\n",
    "            self.recon_img = tf.reshape(X_decoded, shape=(batch_size, self.height, self.width, self.channels))\n",
    "            \n",
    "        accuracy = Accuracy(Y, argmax_idx)\n",
    "        return margin_loss, reconstruction_loss, total_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. 损失函数<a id='Sec4.6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MarginLoss(v, Y, m_plus = 0.9, m_minus = 0.1, lambda_val = 0.5):\n",
    "    '''\n",
    "    Calculate the sum of separate digit margin loss for every samples\n",
    "    and average it over batched samples.\n",
    "    Input:\n",
    "        Y         - One-hot labels.\n",
    "        v         - The output tensor of the DigitCaps layer.   \n",
    "    '''    \n",
    "    epsilon = 1e-9\n",
    "    batch_size = tf.shape(v)[0]\n",
    "    v_length = tf.sqrt(tf.reduce_sum(tf.square(v),\n",
    "                                               axis=2, keepdims=True) + epsilon)\n",
    "    # [batch_size, 10, 1, 1]\n",
    "    # max_l = max(0, m_plus-||v_c||)^2\n",
    "    max_l = tf.square(tf.maximum(0., m_plus - v_length))\n",
    "    # max_r = max(0, ||v_c||-m_minus)^2\n",
    "    max_r = tf.square(tf.maximum(0., v_length - m_minus))    \n",
    "    \n",
    "    # reshape: [batch_size, 10, 1, 1] => [batch_size, 10]\n",
    "    max_l = tf.reshape(max_l, shape=(batch_size, -1))\n",
    "    max_r = tf.reshape(max_r, shape=(batch_size, -1))\n",
    "    \n",
    "    # calc T_c: [batch_size, 10]\n",
    "    T_c = Y\n",
    "    # [batch_size, 10], element-wise multiply\n",
    "    L_c = T_c * max_l + lambda_val * (1 - T_c) * max_r\n",
    "\n",
    "    return tf.reduce_mean(tf.reduce_sum(L_c, axis=1))\n",
    "\n",
    "def ReconstructionLoss(X, X_decoded):\n",
    "    '''\n",
    "    Calculate the sum of squared construction error for every samples\n",
    "    and average it over batched samples.\n",
    "    Input:\n",
    "        X         - Input tensor.\n",
    "        X_decoded - The output tensor of the reconstruction layer.    \n",
    "    '''\n",
    "    batch_size = tf.shape(X)[0]\n",
    "    orgin = tf.reshape(X, shape=(batch_size, -1))\n",
    "    squared = tf.square(X_decoded - orgin)\n",
    "    return tf.reduce_mean(tf.reduce_sum(squared, axis=1))\n",
    "    \n",
    "def Loss(X, Y, v, X_decoded, regularization_scale = 0.0005):\n",
    "    '''\n",
    "    Input:\n",
    "        X         - Input tensor.\n",
    "        Y         - One-hot labels.\n",
    "        v         - The output tensor of the DigitCaps layer.\n",
    "        X_decoded - The output tensor of the reconstruction layer.\n",
    "    '''\n",
    "    margin_loss = MarginLoss(v, Y)\n",
    "    reconstruction_loss = ReconstructionLoss(X, X_decoded)\n",
    "    return margin_loss, reconstruction_loss, margin_loss + regularization_scale * reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(Y, Y_pred):\n",
    "    '''\n",
    "    Input:\n",
    "        Y      - One-hot vectors with shape [BATCH_SIZE, NUM_CLASS]\n",
    "        Y_pred - The predicted classes with shape [BATCH_SIZE, ]\n",
    "    '''\n",
    "    correct_prediction = tf.equal(tf.to_int32(tf.argmax(Y, axis=1)), Y_pred)\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. 优化器<a id='Sec4.7'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用默认参数的Adam优化器\n",
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8. 训练与评估<a id='Sec4.8'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------Epoch  1/10--------------------------------------------\n",
      "Training\n",
      "batch   1: Margin Loss:0.8099, Reconstruction Loss:180.7562, Total Loss:0.9003, Accuracy:0.0781\n",
      "batch 101: Margin Loss:0.6824, Reconstruction Loss:50.6285, Total Loss:0.7077, Accuracy:0.6094\n",
      "batch 201: Margin Loss:0.6435, Reconstruction Loss:40.5060, Total Loss:0.6638, Accuracy:0.6953\n",
      "batch 301: Margin Loss:0.6296, Reconstruction Loss:38.9720, Total Loss:0.6491, Accuracy:0.6016\n",
      "batch 401: Margin Loss:0.6198, Reconstruction Loss:35.9607, Total Loss:0.6378, Accuracy:0.7344\n",
      "Testing:\n",
      "\tMargin Loss:0.6104, Reconstruction Loss:35.7684, Total Loss:0.6283, Accuracy:0.7202\n",
      "--------------------------------------------Epoch  2/10--------------------------------------------\n",
      "Training\n",
      "batch   1: Margin Loss:0.6059, Reconstruction Loss:34.0771, Total Loss:0.6229, Accuracy:0.7500\n",
      "batch 101: Margin Loss:0.6114, Reconstruction Loss:34.1039, Total Loss:0.6285, Accuracy:0.6953\n",
      "batch 201: Margin Loss:0.5876, Reconstruction Loss:30.7336, Total Loss:0.6030, Accuracy:0.7578\n",
      "batch 301: Margin Loss:0.6058, Reconstruction Loss:32.4880, Total Loss:0.6221, Accuracy:0.7031\n",
      "batch 401: Margin Loss:0.5829, Reconstruction Loss:30.0627, Total Loss:0.5979, Accuracy:0.7891\n",
      "Testing:\n",
      "\tMargin Loss:0.5887, Reconstruction Loss:29.7957, Total Loss:0.6036, Accuracy:0.7434\n",
      "--------------------------------------------Epoch  3/10--------------------------------------------\n",
      "Training\n",
      "batch   1: Margin Loss:0.5813, Reconstruction Loss:29.1123, Total Loss:0.5958, Accuracy:0.8047\n",
      "batch 101: Margin Loss:0.5907, Reconstruction Loss:29.9067, Total Loss:0.6057, Accuracy:0.7109\n",
      "batch 201: Margin Loss:0.5877, Reconstruction Loss:28.6457, Total Loss:0.6020, Accuracy:0.6953\n",
      "batch 301: Margin Loss:0.5975, Reconstruction Loss:28.7958, Total Loss:0.6119, Accuracy:0.6719\n",
      "batch 401: Margin Loss:0.5811, Reconstruction Loss:28.0409, Total Loss:0.5951, Accuracy:0.7188\n",
      "Testing:\n",
      "\tMargin Loss:0.5781, Reconstruction Loss:27.1447, Total Loss:0.5917, Accuracy:0.7551\n",
      "--------------------------------------------Epoch  4/10--------------------------------------------\n",
      "Training\n",
      "batch   1: Margin Loss:0.5828, Reconstruction Loss:26.7230, Total Loss:0.5962, Accuracy:0.7344\n",
      "batch 101: Margin Loss:0.5868, Reconstruction Loss:26.1579, Total Loss:0.5999, Accuracy:0.7344\n",
      "batch 201: Margin Loss:0.5672, Reconstruction Loss:26.5614, Total Loss:0.5804, Accuracy:0.8047\n",
      "batch 301: Margin Loss:0.5686, Reconstruction Loss:24.7278, Total Loss:0.5809, Accuracy:0.7891\n",
      "batch 401: Margin Loss:0.5883, Reconstruction Loss:25.7204, Total Loss:0.6011, Accuracy:0.7188\n",
      "Testing:\n",
      "\tMargin Loss:0.5713, Reconstruction Loss:25.3174, Total Loss:0.5840, Accuracy:0.7581\n",
      "--------------------------------------------Epoch  5/10--------------------------------------------\n",
      "Training\n",
      "batch   1: Margin Loss:0.5732, Reconstruction Loss:25.7380, Total Loss:0.5860, Accuracy:0.7578\n",
      "batch 101: Margin Loss:0.5712, Reconstruction Loss:25.2765, Total Loss:0.5838, Accuracy:0.7266\n",
      "batch 201: Margin Loss:0.5746, Reconstruction Loss:26.6917, Total Loss:0.5880, Accuracy:0.8125\n",
      "batch 301: Margin Loss:0.5860, Reconstruction Loss:25.3892, Total Loss:0.5987, Accuracy:0.7266\n",
      "batch 401: Margin Loss:0.5778, Reconstruction Loss:25.5677, Total Loss:0.5906, Accuracy:0.7266\n",
      "Testing:\n",
      "\tMargin Loss:0.5665, Reconstruction Loss:24.2266, Total Loss:0.5786, Accuracy:0.7624\n",
      "--------------------------------------------Epoch  6/10--------------------------------------------\n",
      "Training\n",
      "batch   1: Margin Loss:0.5615, Reconstruction Loss:23.7016, Total Loss:0.5734, Accuracy:0.7656\n",
      "batch 101: Margin Loss:0.5755, Reconstruction Loss:24.4052, Total Loss:0.5877, Accuracy:0.7188\n",
      "batch 201: Margin Loss:0.5551, Reconstruction Loss:22.2530, Total Loss:0.5663, Accuracy:0.7656\n",
      "batch 301: Margin Loss:0.5709, Reconstruction Loss:25.8876, Total Loss:0.5838, Accuracy:0.7500\n",
      "batch 401: Margin Loss:0.5670, Reconstruction Loss:23.5495, Total Loss:0.5788, Accuracy:0.7500\n",
      "Testing:\n",
      "\tMargin Loss:0.5626, Reconstruction Loss:23.4333, Total Loss:0.5744, Accuracy:0.7645\n",
      "--------------------------------------------Epoch  7/10--------------------------------------------\n",
      "Training\n",
      "batch   1: Margin Loss:0.5709, Reconstruction Loss:23.5006, Total Loss:0.5826, Accuracy:0.7656\n",
      "batch 101: Margin Loss:0.5653, Reconstruction Loss:25.1234, Total Loss:0.5779, Accuracy:0.7266\n",
      "batch 201: Margin Loss:0.5710, Reconstruction Loss:24.2608, Total Loss:0.5831, Accuracy:0.7188\n",
      "batch 301: Margin Loss:0.5653, Reconstruction Loss:22.6085, Total Loss:0.5766, Accuracy:0.7891\n",
      "batch 401: Margin Loss:0.5681, Reconstruction Loss:23.5656, Total Loss:0.5799, Accuracy:0.7031\n",
      "Testing:\n",
      "\tMargin Loss:0.5597, Reconstruction Loss:22.8300, Total Loss:0.5711, Accuracy:0.7667\n",
      "--------------------------------------------Epoch  8/10--------------------------------------------\n",
      "Training\n",
      "batch   1: Margin Loss:0.5638, Reconstruction Loss:22.9444, Total Loss:0.5753, Accuracy:0.7500\n",
      "batch 101: Margin Loss:0.5849, Reconstruction Loss:25.3915, Total Loss:0.5976, Accuracy:0.6953\n",
      "batch 201: Margin Loss:0.5594, Reconstruction Loss:22.4024, Total Loss:0.5706, Accuracy:0.7734\n",
      "batch 301: Margin Loss:0.5566, Reconstruction Loss:21.6458, Total Loss:0.5674, Accuracy:0.7578\n",
      "batch 401: Margin Loss:0.5425, Reconstruction Loss:21.3778, Total Loss:0.5532, Accuracy:0.7812\n",
      "Testing:\n",
      "\tMargin Loss:0.5568, Reconstruction Loss:22.3995, Total Loss:0.5680, Accuracy:0.7674\n",
      "--------------------------------------------Epoch  9/10--------------------------------------------\n",
      "Training\n",
      "batch   1: Margin Loss:0.5494, Reconstruction Loss:21.2890, Total Loss:0.5601, Accuracy:0.7812\n",
      "batch 101: Margin Loss:0.5458, Reconstruction Loss:21.8888, Total Loss:0.5567, Accuracy:0.7734\n",
      "batch 201: Margin Loss:0.5594, Reconstruction Loss:22.3730, Total Loss:0.5706, Accuracy:0.7734\n",
      "batch 301: Margin Loss:0.5616, Reconstruction Loss:21.9750, Total Loss:0.5726, Accuracy:0.7656\n",
      "batch 401: Margin Loss:0.5479, Reconstruction Loss:21.4499, Total Loss:0.5586, Accuracy:0.7734\n",
      "Testing:\n",
      "\tMargin Loss:0.5546, Reconstruction Loss:21.9351, Total Loss:0.5656, Accuracy:0.7687\n",
      "--------------------------------------------Epoch 10/10--------------------------------------------\n",
      "Training\n",
      "batch   1: Margin Loss:0.5525, Reconstruction Loss:22.0651, Total Loss:0.5635, Accuracy:0.7578\n",
      "batch 101: Margin Loss:0.5630, Reconstruction Loss:22.6947, Total Loss:0.5743, Accuracy:0.7422\n",
      "batch 201: Margin Loss:0.5758, Reconstruction Loss:23.6276, Total Loss:0.5876, Accuracy:0.7188\n",
      "batch 301: Margin Loss:0.5625, Reconstruction Loss:22.1713, Total Loss:0.5736, Accuracy:0.7656\n",
      "batch 401: Margin Loss:0.5530, Reconstruction Loss:21.2477, Total Loss:0.5636, Accuracy:0.7734\n",
      "Testing:\n",
      "\tMargin Loss:0.5528, Reconstruction Loss:21.5064, Total Loss:0.5635, Accuracy:0.7715\n"
     ]
    }
   ],
   "source": [
    "# 确保有足够的GPU资源，否则应调整减少batch size，\n",
    "# 或者使用CPU进行训练\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "model = CapsNet()\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    print(\"--------------------------------------------Epoch %2d/%2d--------------------------------------------\" % (epoch+1, NUM_EPOCH))\n",
    "    print('Training')\n",
    "    for (batch, (x, y)) in enumerate(TrainDataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            margin_loss, reconstruction_loss, total_loss, accuracy = model(x,y)\n",
    "        grads = tape.gradient(model.total_loss, model.variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables))\n",
    "        if batch%100==0:\n",
    "            print('batch %3d:'%(batch+1),'Margin Loss:%3.4f, Reconstruction Loss:%3.4f, Total Loss:%3.4f, Accuracy:%3.4f'%(margin_loss, reconstruction_loss, total_loss, accuracy))\n",
    "    \n",
    "    margin_loss_list = []\n",
    "    reconstruction_loss_list = []\n",
    "    total_loss_list =[]\n",
    "    accuracy_list = []\n",
    "    for (batch, (x, y)) in enumerate(TestDataset):\n",
    "        margin_loss, reconstruction_loss, total_loss, accuracy = model(x,y)\n",
    "        margin_loss_list.append(margin_loss)\n",
    "        reconstruction_loss_list.append(reconstruction_loss)\n",
    "        total_loss_list.append(total_loss)\n",
    "        accuracy_list.append(accuracy)\n",
    "    print('Testing:')\n",
    "    print('\\tMargin Loss:%3.4f, Reconstruction Loss:%3.4f, Total Loss:%3.4f, Accuracy:%3.4f'%(np.mean(margin_loss_list), np.mean(reconstruction_loss_list), np.mean(total_loss_list), np.mean(accuracy_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x144 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x144 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 可视化最后一个测试集批次的前4个样本\n",
    "fig1 = plt.figure(figsize=(8,2))\n",
    "for i in range(4):\n",
    "    fig1.add_subplot(1,4,i+1)\n",
    "    plt.imshow(np.reshape(x[i:i+1],(28,28)), cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.axis('off')\n",
    "fig1.suptitle('Real Images')\n",
    "fig2 = plt.figure(figsize=(8,2))   \n",
    "fig2.suptitle('Reconstructed Images')\n",
    "for i in range(4):    \n",
    "    fig2.add_subplot(1,4,i+1)\n",
    "    model(x[i:i+1], y[i:i+1], reconstruct=True)\n",
    "    plt.imshow(np.reshape(model.recon_img,(28,28)), cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
